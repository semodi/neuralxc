#!/usr/bin/python3
import argparse

from neuralxc.drivers import *

if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Add data to hdf5 file')
    subparser = parser.add_subparsers()

    #================ Plot Basis Set ================
    basis = subparser.add_parser('basis', description='Plot radial basis functions')
    basis.add_argument('basis',
                       action='store',
                       type=str,
                       help='Path to .json file \
        containing the basis to plot')
    basis.set_defaults(func=plot_basis)

    # =======================================================
    # =============== Data routines =========================
    # =======================================================
    dat = subparser.add_parser('data', description='Routines to manipulate datasets')
    datsub = dat.add_subparsers()

    #================ Add data ================
    adddat = datsub.add_parser('add', description='Add data to hdf5 file')
    adddat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    adddat.add_argument('system', action='store', type=str, help='System')
    adddat.add_argument('method', action='store', type=str, help='Method')
    adddat.add_argument('add',
                        action='store',
                        type=str,
                        nargs='*',
                        help='Which quantities to add (energy, forces, density)')
    adddat.add_argument('--traj', metavar='traj', type=str, default='', help='Path to .xyz/.traj file')
    adddat.add_argument('--density', metavar='density', type=str, default='', help='Path to basis representation file')
    adddat.add_argument('--override', action=('store_true'), help='If exists, override? (Default: False)')
    adddat.add_argument('--slice', metavar='slice', type=str, default=':', help='Only add slice of dataset')
    adddat.add_argument('--zero',
                        metavar='zero',
                        type=float,
                        default=None,
                        help='Shift energies by this value, if not set, use minimum of dataset.')
    adddat.add_argument('--addto',
                        metavar='addto',
                        type=str,
                        default='',
                        help='If adding energies add to these energies in hdf5 file')
    adddat.set_defaults(func=add_data_driver)

    def inspectdat_driver(args):
        subprocess.Popen('h5dump -n ' + args.hdf5, shell=True)

    #================ Inspect data ================
    inspectdat = datsub.add_parser('inspect', description='Inspect data in hdf5 file')
    inspectdat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    inspectdat.set_defaults(func=inspectdat_driver)

    #================ Split data ================
    splitdat = datsub.add_parser('split', description='Split dataset (e.g. into training and test set)')
    splitdat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    splitdat.add_argument('group', action='store', type=str, help='Which group to apply slicing to')
    splitdat.add_argument('label', action='store', type=str, help='New label for slice')
    splitdat.add_argument('--slice', metavar='slice', default=':', type=str, help='Slice in numpy notation')
    splitdat.add_argument('--comp',
                          metavar='comp',
                          default='',
                          type=str,
                          help='Store complementary slice under this group')
    splitdat.set_defaults(func=split_data_driver)

    #================ Delete data ================
    deldat = datsub.add_parser('delete', description='Delete group inside hdf5 file')
    deldat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    deldat.add_argument('group', action='store', type=str, help='Which group to delete')
    deldat.set_defaults(func=delete_data_driver)

    #================ Sample data ================
    sampledat = datsub.add_parser(
        'sample', description='Sample the data for a given basis set using KMeans cluster in feature-space')
    sampledat.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    sampledat.add_argument('size', action='store', type=int, help='Sample size')
    sampledat.add_argument('--dest',
                           action='store',
                           type=str,
                           default='sample.npy',
                           help='Save to (default: sample.npy)')
    sampledat.add_argument('--hdf5', metavar='hdf5', type=str, nargs=2, help='Path to hdf5 file, baseline data')
    sampledat.add_argument('--cutoff', metavar='cutoff', type=float, default=0.0, help='Cut off extreme datapoints')
    sampledat.set_defaults(func=sample_driver)

    #================ Merge datasets =============
    mergedat = datsub.add_parser('merge', description='Merge datasets inside hdf5 file')
    mergedat.add_argument('file', action='store', type=str, help='Path to hdf5 file')
    mergedat.add_argument('--base', action='store', type=str, nargs='+', help='Baseline sets', required=True)
    mergedat.add_argument('--ref', action='store', type=str, nargs='+', help='Reference sets', required=True)
    mergedat.add_argument('--out', action='store', type=str, help='Destination', required=True)
    mergedat.add_argument('--optE0', action='store_true', help='Optimize energy offset across datasets (recommended)')
    mergedat.add_argument('--pre', action='store', default='', type=str, help='Preprocessor file defining basis set')

    mergedat.set_defaults(func=merge_data_driver)

    # =======================================================
    # =============== Model routines ========================
    # =======================================================

    # =============== Fitter =====================

    fit = subparser.add_parser('fit', description='Fit a NeuralXC model')
    fit.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    fit.add_argument('hyper', action='store', help='Path to .json configuration file setting hyperparameters')
    fit.add_argument('--hdf5',
                     metavar='hdf5',
                     type=str,
                     nargs=3,
                     help='Path to hdf5 file, baseline data, reference data')
    fit.add_argument('--sets', metavar='sets', type=str, default='', help='Path to file defining sets')
    fit.add_argument('--sample',
                     metavar='sample',
                     type=str,
                     default='',
                     help='Only use a subsample of data contained in hdf5 file')
    fit.add_argument('--cutoff', metavar='cutoff', type=float, default=0.0, help='Cut off extreme datapoints')
    fit.add_argument('--model',
                     metavar='model',
                     type=str,
                     default='',
                     help='Continue training model found at this location')
    fit.add_argument('--hyperopt', action='store_true', help='Do a hyperparameter optimzation')
    fit.set_defaults(func=fit_driver)

    # =============  Selfconsistent  ====================
    ad = subparser.add_parser('sc', description='Fit a NeuralXC model selfconsistently')
    ad.add_argument('xyz', action='store', help='Path to .xyz/.traj file containing structures and reference data')
    ad.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    ad.add_argument('hyper', action='store', help='Path to .json configuration file setting hyperparameters')
    ad.add_argument('--data',
                    metavar='data',
                    type=str,
                    default='',
                    help='Start from this dataset instead of computing iteration 0')
    ad.add_argument('--maxit',
                    metavar='maxit',
                    type=int,
                    default='5',
                    help='Maximum number of iterations (default: 5)')
    ad.add_argument('--tol',
                    metavar='tol',
                    type=float,
                    default='0.0005',
                    help='Tolerance in energy defining whether iterative training converged (default: 0.0005 eV)')
    ad.add_argument('--sets', metavar='sets', type=str, default='', help='Path to file defining sets')
    ad.add_argument('--nozero',
                    action='store_true',
                    help='Do not automatically set energy origins for every dataset by using min')
    ad.add_argument('--model0',
                    metavar='model0',
                    type=str,
                    default='',
                    help='Build new model on top of model0 as a stacked estimator')
    ad.add_argument('--hyperopt', action='store_true', help='Do a hyperparameter optimzation')
    ad.set_defaults(func=sc_driver)

    # =============== Evaluate =====================

    eval = subparser.add_parser('eval', description='Evaluate a NeuralXC model')
    eval.add_argument('--model', metavar='model', default='', help='Path to NeuralXC model')
    eval.add_argument('--hdf5',
                      metavar='hdf5',
                      type=str,
                      nargs=3,
                      help='Path to hdf5 file, baseline data, reference data')
    eval.add_argument('--plot', action='store_true', help='Create scatterplot?')
    eval.add_argument('--savefig', action='store', type=str, default='', help='Save scatterplot?')
    eval.add_argument('--cutoff', metavar='cutoff', type=float, default=0.0, help='Cut off extreme datapoints')
    eval.add_argument('--sample',
                      action='store',
                      metavar='sample',
                      type=str,
                      default='',
                      help='Evaluate on sample. Path to sample file')
    eval.add_argument('--invert_sample',
                      action='store_true',
                      help='Invert the sample provided (evaluate on datapoints not in sample)')
    eval.add_argument('--keep_mean',
                      action='store_true',
                      help="If set, don't subract parallelity error from MAE and RMSE")
    eval.add_argument('--hashkey',
                      action='store',
                      default='',
                      help="Manually choose which basis hash key to apply model to")
    eval.set_defaults(predict=False)
    eval.set_defaults(func=eval_driver)

    # =============== Predict =====================

    pred = subparser.add_parser('predict', description='Predict energies with NeuralXC model')
    pred.add_argument('--model', metavar='model', help='Path to NeuralXC model')
    pred.add_argument('--hdf5',
                      metavar='hdf5',
                      type=str,
                      nargs=2,
                      help='Path to hdf5 file, baseline data, reference data')
    pred.add_argument('--dest', metavar='dest', type=str, default='prediction', help='Destination where to store data')
    pred.add_argument('--hashkey',
                      action='store',
                      default='',
                      help="Manually choose which basis hash key to apply model to")
    pred.set_defaults(predict=True)
    pred.set_defaults(func=eval_driver)

    #================ Compile model ==========

    jitcon = subparser.add_parser('serialize', description='Converts model to TorchScript')
    jitcon.add_argument('in_path', action='store', help='Path to model')
    jitcon.add_argument('jit_path', action='store', help='Destination for TorchScript model')
    jitcon.add_argument('--as_radial', action=('store_true'), help='Save as model that works on radial grids?')
    jitcon.set_defaults(func=serialize)

    # ======================================================
    # =============== Preprocessor ========================
    # =======================================================
    pre = subparser.add_parser('pre', description='Preprocess electron density')
    pre.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    pre.add_argument('--dest',
                     metavar='dest',
                     type=str,
                     default='.tmp/',
                     help='Destination where to store data,\
                                                                        can be either a directory or an .hdf5 file (with groups) (default: .tmp/)'
                     )
    pre.add_argument('--xyz', metavar='xyz', type=str, default='', help='Path to xyz file')
    pre.add_argument('--srcdir', metavar='srcdir', type=str, default='.', help='Source directory containing densities')
    pre.set_defaults(func=pre_driver)

    df = subparser.add_parser('default', description='Fetch default configuration files')
    df.add_argument('kind',
                    action='store',
                    help='Preprocessor (pre) or hyperparameter (hyper) config file',
                    choices=['pre', 'hyper'])
    df.add_argument('--hint',
                    metavar='hint',
                    type=str,
                    default='',
                    help='Partially complete config file to fill with defaults')
    df.add_argument('--out', metavar='out', type=str, default='', help='Store to (default pre.json/hyper.json)')
    df.set_defaults(func=fetch_default_driver)

    eng = subparser.add_parser('engine', description='Run engine for structures stored in .xyz/.traj file')
    eng.add_argument('preprocessor', metavar='preprocessor', type=str, help='Config file for preprocessor')
    eng.add_argument('xyz', metavar='xyz', type=str, help='.xyz or .traj file containing structures')
    eng.add_argument(
        '--workdir',
        metavar='workdir',
        type=str,
        default='.tmp/',
        help='Specify work-directory. If not specified uses .tmp/ and deletes after calculation has finished')
    eng.set_defaults(func=run_engine_driver)

    args = parser.parse_args()

    args_dict = args.__dict__
    func = args_dict.pop('func')

    # args.func(args)
    func(**args_dict)
